## Data Source

Dataset is generated by Visual Geometry Group of University of Oxford
This is synthetically generated dataset which found to be sufficient for training text recognition on real-world images
This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in authors work.
(archived dataset is about 10 GB)

Please read more information here https://www.robots.ox.ac.uk/~vgg/data/text/


## File structure

File sturcture withing tar.gz:

mnt/ramdisk/max/90kDICT32px/<directory_as_number>/<directory_as_number>/<file_name_with_textLabel>.jpg     
mnt/ramdisk/max/90kDICT32px/<some_text_files_about_data>.txt

## Converting to other formats

This process is done using slurm session (it is expensive computation, and in SLURM sesssion SLURM_TMPDIR does not have the same limit on number of files, as /scratch)

### How data is stored:

*Images:* 

* lmdb

contains images in binary jpg format. These objects can be read from any language: python, R, MATLAB, etc.

lmdb keys have 7 digits starting from "0000001", for example image number 12 has key "0000012"

* hdf5

contains images in binary jpg format. These objects can be read from any language: python, R, MATLAB, etc.

hdf5 dataset name is "images"    
hdf5 elements index start from 0. 

* lmdb_numpy

contains image converted to NumPy and saved as pickle  

Not all images were converted succesfully. 32 Files had errors.Errors are stored in file errors_lmdb.txt


*Metadata:*

Metadata is stored in SQLite file

lmdb SQLite file schema: 
* key - lmdb key
* label - word extracted from filename
* path - path within original tar.gz archive

hdf5 SQLite file schema: 
* key - file number (starts from 0)
* label - word extracted from filename
* path - path within original tar.gz archive

lmdb_numpy SQLite file schema: 
* key - lmdb key
* label - word extracted from filename
* path - path within original tar.gz archive


## Files

### slurm sbatch script

slurm_send_writeAll - batch file for slurm job

### lmdb

Look at file write_jpg_lmdb_all.py

### hdf5

Look at file write_jpg_hdf5_all.py


## Some data

### how long it took

* lmdb: 1297 sec (~ 20 min)
* hdf5: 3523 sec (~ 1 hour)
* lmdb_numpy: 6055 sec (~ 100 min)

### how big are the files

$ du -h lmdb/TextImages*
26G     lmdb/TextImages.lmdb
624M    lmdb/TextImages.sqlite

$ du -h hdf5/TextImages*
15G     hdf5/TextImages.hdf5
407M    hdf5/TextImages-hdf5.sqlite

$ du -sh lmdb_numpy/*
8.0K    lmdb_numpy/errors_lmdb.txt
109G    lmdb_numpy/TextImages_numpy.lmdb
624M    lmdb_numpy/TextImages_numpy.sqlite

### lmdb tree info

env = lmdb.open("/scratch/work/public/datasets/TextRecognitionData_VGG_Oxford/lmdb/TextImages.lmdb", readonly=True, lock=False)

env.stat()

{'psize': 4096, 'depth': 4, 'branch_pages': 21444, 'leaf_pages': 4824594, 'overflow_pages': 1874918, 'entries': 8919273}

### lmdb_numpy tree info

env = lmdb.open("/scratch/work/public/datasets/TextRecognitionData_VGG_Oxford/lmdb_numpy/TextImages_numpy.lmdb", readonly=True, lock=False)

env.stat()

{'psize': 4096, 'depth': 4, 'branch_pages': 262, 'leaf_pages': 58516, 'overflow_pages': 28402217, 'entries': 8919241}



```{bash}
module load anaconda3/5.3.1
conda create -p $(pwd)/penv python=3.7
conda activate ...
conda install -y pillow hdf5 h5py matplotlib numpy pathlib lmdb python-lmdb sqlite pandas
```
